"""
ì›¹ ìŠ¤í¬ë˜í•‘ì„ ìˆ˜í–‰í•˜ëŠ” í•µì‹¬ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ì§€ì •ëœ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê²Œì‹œë¬¼ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³ 
ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ì›¹ í˜ì´ì§€ ìš”ì²­ ë° ì‘ë‹µ ì²˜ë¦¬
- HTML íŒŒì‹±
- ë°ì´í„° ì¶”ì¶œ ë° ì €ì¥
"""

from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import json
from scraper.utils.user_agent import get_random_user_agent
from scraper.utils.logger import get_logger
from scraper.core.database import get_db
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
import time
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import re
from scraper.utils.trans_desc import translate_to_korean
from typing import Optional, List, Dict, Any
import threading

# ìƒìˆ˜ ì •ì˜
BASE_URL = "https://onejav.com"
WAIT_TIME = 0.3  # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸° ì‹œê°„
PARSE_DELAY = 0.05  # ê²Œì‹œë¬¼ë³„ íŒŒì‹± ê°„ ëŒ€ê¸° ì‹œê°„
MAX_RETRIES = 3  # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜

logger = get_logger(__name__)

class Scraper:
    """ì›¹ ìŠ¤í¬ë˜í•‘ì„ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, target_url: str):
        """Scraper ì´ˆê¸°í™”"""
        self.target_url = target_url
        self.db = get_db()
        self.lock = threading.Lock()  # ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•œ ë½ ì¶”ê°€
        
        # Selenium WebDriver ì„¤ì •
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument(f'user-agent={get_random_user_agent()}')
        self.driver = webdriver.Chrome(options=chrome_options)
        self.wait = WebDriverWait(self.driver, WAIT_TIME)

    def get_page_with_selenium(self, url: str) -> BeautifulSoup:
        """Seleniumì„ ì‚¬ìš©í•˜ì—¬ í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜´"""
        for attempt in range(MAX_RETRIES):
            try:
                print(f"ğŸŒ í˜ì´ì§€ ìš”ì²­: {url}")
                self.driver.get(url)
                self.wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
                time.sleep(WAIT_TIME)
                
                # í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ ì²´í¬
                try:
                    self.wait.until(lambda driver: driver.execute_script('return document.readyState') == 'complete')
                except Exception as e:
                    print(f"âš¡ í˜ì´ì§€ ë¡œë“œ ì˜¤ë¥˜: {e}")
                    raise
                
                # ì»¨í…Œì´ë„ˆ ìš”ì†Œ ì²´í¬
                try:
                    self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, "container")))
                except Exception as e:
                    print(f"âš¡ ì»¨í…Œì´ë„ˆ ë¡œë“œ ì˜¤ë¥˜: {e}")
                    raise
                
                return BeautifulSoup(self.driver.page_source, 'html.parser')
            except Exception as e:
                print(f"âš¡ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{MAX_RETRIES}): {str(e)}")
                if attempt == MAX_RETRIES - 1:
                    raise
                time.sleep(WAIT_TIME)

    def is_duplicate(self, post_data: dict) -> bool:
        """ê²Œì‹œë¬¼ì´ ì´ë¯¸ DBì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        try:
            existing_post = self.db.get_post_by_url(post_data['url'])
            return existing_post is not None
        except Exception as e:
            print(f"âš¡ ì¤‘ë³µ ì²´í¬ ì˜¤ë¥˜: {str(e)}")
            return False

    def process_card(self, card) -> Optional[dict]:
        """ì¹´ë“œì—ì„œ ê²Œì‹œë¬¼ ë°ì´í„° ì¶”ì¶œ"""
        try:
            date_elem = card.find('p', class_='subtitle').find('a')
            if not date_elem:
                return None
            post_date_str = date_elem.text.strip()
            try:
                post_date = datetime.strptime(post_date_str, '%B %d, %Y')
            except ValueError:
                post_date = datetime.now()

            title_elem = card.find('h5', class_='title').find('a')
            if not title_elem:
                return None
            title = title_elem.text.strip()
            
            # ì œëª© í˜•ì‹ ë³€í™˜
            m_fc2 = re.match(r'^(FC2)PPV(\d+)$', title, re.IGNORECASE)
            if m_fc2:
                title = f"{m_fc2.group(1).upper()}-PPV-{m_fc2.group(2)}"
            else:
                m_3prefix = re.match(r'^(\d{3})([A-Za-z]+)(\d+)$', title)
                if m_3prefix:
                    title = f"{m_3prefix.group(1)}{m_3prefix.group(2)}-{m_3prefix.group(3)}"
                elif len(title) <= 8:
                    m = re.match(r'^([A-Za-z]+)(\d+)$', title)
                    if m:
                        title = f"{m.group(1)}-{m.group(2)}"

            post_url = title_elem['href']
            if not post_url.startswith('http'):
                post_url = f"{BASE_URL}{post_url}"

            img_elem = card.find('img', class_='image')
            if not img_elem or 'src' not in img_elem.attrs:
                return None
            image_url = img_elem['src']

            size_elem = card.find('span', class_='is-size-6')
            if not size_elem:
                return None
            file_size = size_elem.text.strip()

            tag_elems = card.find_all('a', class_='tag')
            tags = [tag.text.strip() for tag in tag_elems]

            download_elem = card.find('a', class_='button is-primary is-fullwidth')
            if not download_elem or 'href' not in download_elem.attrs:
                return None
            download_url = download_elem['href']
            if not download_url.startswith('http'):
                download_url = f"{BASE_URL}{download_url}"

            # description ì¶”ì¶œ
            description = ""
            desc_elem = card.find('p', class_='level has-text-grey-dark')
            if desc_elem:
                description = desc_elem.text.strip()

            # actress ì¶”ì¶œ
            actress = []
            panel_elem = card.find('div', class_='panel')
            if panel_elem:
                actress = [a.text.strip() for a in panel_elem.find_all('a', class_='panel-block')]

            return {
                'url': post_url,
                'code': title,
                'title': title,
                'image_url': image_url,
                'file_size': file_size,
                'post_date': post_date,
                'tags': json.dumps(tags),
                'description': description,
                'translated_desc': "",  # ë²ˆì—­ì€ ë‚˜ì¤‘ì— ìˆ˜í–‰
                'actress': json.dumps(actress, ensure_ascii=False),
                'download_url': download_url,
                'scraped_at': datetime.now()
            }
        except Exception as e:
            print(f"âš¡ ê²Œì‹œë¬¼ ì‘ì—… ì˜¤ë¥˜: {str(e)}")
            return None

    def save_post(self, post_data: dict, skip_duplicate_check: bool = False):
        """ê²Œì‹œë¬¼ ë°ì´í„°ë¥¼ DBì— ì €ì¥"""
        try:
            # ì¤‘ë³µ ì²´í¬ (skip_duplicate_checkê°€ Falseì¼ ë•Œë§Œ)
            if not skip_duplicate_check and self.is_duplicate(post_data):
                return False

            # ë²ˆì—­ ìˆ˜í–‰
            if post_data['description']:
                try:
                    post_data['translated_desc'] = translate_to_korean(post_data['description'])
                    # ë²ˆì—­ ì„±ê³µ ë©”ì‹œì§€ëŠ” ì €ì¥ ì„±ê³µê³¼ í•¨ê»˜ ì¶œë ¥
                except Exception as e:
                    print(f"âš¡ ë²ˆì—­ ì˜¤ë¥˜: {e}")
                    post_data['translated_desc'] = ""

            # ì €ì¥
            with self.lock:  # ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•œ ë½ ì‚¬ìš©
                if self.db.add_post(post_data):
                    print(f"ğŸ’¾ ì €ì¥ ì„±ê³µ: {post_data['title']}")
                    return True
                else:
                    print(f"â— ì €ì¥ ì‹¤íŒ¨: {post_data['title']}")
                    return False
        except Exception as e:
            print(f"âš¡ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
            raise

    def get_next_page_url(self, soup: BeautifulSoup) -> Optional[str]:
        """ë‹¤ìŒ í˜ì´ì§€ URLì„ ì¶”ì¶œ"""
        try:
            pagination = soup.find('nav', class_='pagination')
            if not pagination:
                print("ğŸ í˜ì´ì§€ë„¤ì´ì…˜ ì—†ìŒ")
                return None
            next_link = pagination.find('a', class_='pagination-next')
            if not next_link or 'href' not in next_link.attrs:
                print("ğŸ ë‹¤ìŒ í˜ì´ì§€ ë§í¬ ì—†ìŒ")
                return None
            next_url = next_link['href']
            if not next_url.startswith('http'):
                if next_url.startswith('?'):
                    next_url = f"{BASE_URL}/new{next_url}"
                else:
                    next_url = f"{BASE_URL}{next_url}"
            print(f"ğŸ‘‰ ë‹¤ìŒ í˜ì´ì§€: {next_url}")
            return next_url
        except Exception as e:
            print(f"âš¡ URL ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
        return None

    def scrape_new_posts(self):
        """ìƒˆë¡œìš´ ê²Œì‹œë¬¼ì„ ìŠ¤í¬ë˜í•‘"""
        start_time = time.time()
        start_dt = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print(f"âœ¨ ìŠ¤í¬ë˜í•‘ ì‹œì‘ {start_dt}")
        current_url = f"{BASE_URL}/new"
        today = datetime.now().date()
        should_continue = True
        all_posts = []
        duplicate_count = 0
        saved_count = 0
        skipped_check_count = 0  # ì¤‘ë³µ ì²´í¬ ê±´ë„ˆë›´ ê²Œì‹œë¬¼ ìˆ˜
        translated_count = 0  # ë²ˆì—­í•œ ê²Œì‹œë¬¼ ìˆ˜
        found_new_post = False  # ìƒˆë¡œìš´ ê²Œì‹œë¬¼ ë°œê²¬ ì—¬ë¶€
        current_page = 1  # í˜„ì¬ í˜ì´ì§€ ë²ˆí˜¸
        
        while current_url and should_continue:
            print(f"ğŸ“„ í˜ì´ì§€ ì‘ì—…: {current_url}")
            soup = self.get_page_with_selenium(current_url)
            
            cards = soup.find_all('div', class_='card mb-3')
            print(f"ğŸ§© ë°œê²¬ëœ ê²Œì‹œë¬¼: {len(cards)}ê°œ")
            
            # ìˆœì„œëŒ€ë¡œ ê²Œì‹œë¬¼ ë°ì´í„° ì¶”ì¶œ
            page_posts = []
            for card in cards:
                post_data = self.process_card(card)
                if post_data:
                    if post_data['post_date'].date() < today:
                        should_continue = False
                        print(f"â° ì˜¤ëŠ˜ ì´ì „ ê²Œì‹œë¬¼ ë°œê²¬ (í˜ì´ì§€ {current_page}, ê²Œì‹œë¬¼: {post_data['title']})")
                        break
                    page_posts.append(post_data)
                    print(f"ğŸ“ ê²Œì‹œë¬¼ ì‘ì—…: {post_data['title']}")
            
            all_posts.extend(page_posts)
            if not should_continue:
                break
                
            current_url = self.get_next_page_url(soup)
            if not current_url:
                print("ğŸ ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬")
                break
            current_page += 1  # í˜ì´ì§€ ë²ˆí˜¸ ì¦ê°€

        # ëª¨ë“  ê²Œì‹œë¬¼ ì €ì¥ (ì¤‘ë³µ ì²´í¬ ìˆ˜í–‰, ì—­ìˆœìœ¼ë¡œ ì²˜ë¦¬)
        today_post_count = len(all_posts)  # ì˜¤ëŠ˜ ê²Œì‹œë¬¼ ìˆ˜
        for post_data in reversed(all_posts):  # ì—­ìˆœìœ¼ë¡œ ì²˜ë¦¬
            if not found_new_post:
                if self.save_post(post_data, skip_duplicate_check=False):
                    saved_count += 1
                    if post_data['description']:
                        translated_count += 1
                    found_new_post = True
                else:
                    duplicate_count += 1
            else:
                if self.save_post(post_data, skip_duplicate_check=True):
                    saved_count += 1
                    if post_data['description']:
                        translated_count += 1
                    skipped_check_count += 1
        end_time = time.time()
        end_dt = datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        elapsed = end_time - start_time
        print(f"ğŸ—“ï¸ ì˜¤ëŠ˜ ê²Œì‹œë¬¼: {today_post_count}ê°œ")
        print(f"ğŸ” ì¤‘ë³µ ê²Œì‹œë¬¼: {duplicate_count}ê°œ (ì¤‘ë³µ ì²´í¬ ê±´ë„ˆë›´ ê²Œì‹œë¬¼: {skipped_check_count}ê°œ)")
        print(f"ğŸ’¾ ì €ì¥ëœ ê²Œì‹œë¬¼: {saved_count}ê°œ (ë²ˆì—­ëœ ê²Œì‹œë¬¼: {translated_count}ê°œ)")
        print(f"ğŸ‰ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ {end_dt}")
        print(f"â³ ì†Œìš” ì‹œê°„: {elapsed:.1f}ì´ˆ")

    def close(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        if hasattr(self, 'driver'):
            self.driver.quit() 