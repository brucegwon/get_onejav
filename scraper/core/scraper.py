"""
ì›¹ ìŠ¤í¬ë˜í•‘ì„ ìˆ˜í–‰í•˜ëŠ” í•µì‹¬ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ì§€ì •ëœ ì›¹ì‚¬ì´íŠ¸ì—ì„œ ê²Œì‹œë¬¼ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ê³ 
ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•˜ëŠ” ê¸°ëŠ¥ì„ ì œê³µí•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- ì›¹ í˜ì´ì§€ ìš”ì²­ ë° ì‘ë‹µ ì²˜ë¦¬
- HTML íŒŒì‹±
- ë°ì´í„° ì¶”ì¶œ ë° ì €ì¥
"""

from bs4 import BeautifulSoup
from datetime import datetime, timedelta
import json
from scraper.utils.user_agent import get_random_user_agent
from scraper.utils.logger import get_logger
from scraper.core.database import get_db
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
import time
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import re
from scraper.utils.trans_desc import translate_to_korean
from typing import Optional, List, Dict, Any
from concurrent.futures import ThreadPoolExecutor, as_completed

# ìƒìˆ˜ ì •ì˜
BASE_URL = "https://onejav.com"
WAIT_TIME = 0.3  # í˜ì´ì§€ ë¡œë“œ ëŒ€ê¸° ì‹œê°„ (1ì´ˆ â†’ 0.3ì´ˆ)
PARSE_DELAY = 0.05  # ê²Œì‹œë¬¼ë³„ íŒŒì‹± ê°„ ëŒ€ê¸° ì‹œê°„ (1ì´ˆ â†’ 0.05ì´ˆ)
MAX_RETRIES = 3  # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
MAX_WORKERS = 5  # ìµœëŒ€ ë³‘ë ¬ ì‘ì—… ìˆ˜

logger = get_logger(__name__)

class Scraper:
    """ì›¹ ìŠ¤í¬ë˜í•‘ì„ ìˆ˜í–‰í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, target_url: str):
        """Scraper ì´ˆê¸°í™”"""
        self.target_url = target_url
        self.db = get_db()
        
        # Selenium WebDriver ì„¤ì •
        chrome_options = Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--disable-gpu')
        chrome_options.add_argument('--no-sandbox')
        chrome_options.add_argument(f'user-agent={get_random_user_agent()}')
        self.driver = webdriver.Chrome(options=chrome_options)
        self.wait = WebDriverWait(self.driver, WAIT_TIME)

    def get_page_with_selenium(self, url: str) -> BeautifulSoup:
        """Seleniumì„ ì‚¬ìš©í•˜ì—¬ í˜ì´ì§€ë¥¼ ê°€ì ¸ì˜´"""
        for attempt in range(MAX_RETRIES):
            try:
                print(f"ğŸŒŠ í˜ì´ì§€ ìš”ì²­: {url}")
                self.driver.get(url)
                self.wait.until(EC.presence_of_element_located((By.TAG_NAME, "body")))
                time.sleep(WAIT_TIME)
                
                # í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ ì²´í¬
                try:
                    self.wait.until(lambda driver: driver.execute_script('return document.readyState') == 'complete')
                except Exception as e:
                    print(f"ğŸ’¥ í˜ì´ì§€ ë¡œë“œ ì˜¤ë¥˜: {e}")
                    raise
                
                # ì»¨í…Œì´ë„ˆ ìš”ì†Œ ì²´í¬
                try:
                    self.wait.until(EC.presence_of_element_located((By.CLASS_NAME, "container")))
                except Exception as e:
                    print(f"ğŸ’¥ ì»¨í…Œì´ë„ˆ ë¡œë“œ ì˜¤ë¥˜: {e}")
                    raise
                
                return BeautifulSoup(self.driver.page_source, 'html.parser')
            except Exception as e:
                print(f"ğŸ’¥ í˜ì´ì§€ ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{MAX_RETRIES}): {str(e)}")
                if attempt == MAX_RETRIES - 1:
                    raise
                time.sleep(WAIT_TIME)

    def is_duplicate(self, post_data: dict) -> bool:
        """ê²Œì‹œë¬¼ì´ ì´ë¯¸ DBì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        try:
            existing_post = self.db.get_post_by_url(post_data['url'])
            return existing_post is not None
        except Exception as e:
            print(f"ğŸ’¥ ì¤‘ë³µ ì²´í¬ ì˜¤ë¥˜: {str(e)}")
            return False

    def process_card(self, card) -> Optional[dict]:
        """ì¹´ë“œì—ì„œ ê²Œì‹œë¬¼ ë°ì´í„° ì¶”ì¶œ"""
        try:
            date_elem = card.find('p', class_='subtitle').find('a')
            if not date_elem:
                return None
            post_date_str = date_elem.text.strip()
            try:
                post_date = datetime.strptime(post_date_str, '%B %d, %Y')
            except ValueError:
                post_date = datetime.now()

            title_elem = card.find('h5', class_='title').find('a')
            if not title_elem:
                return None
            title = title_elem.text.strip()
            
            # ì œëª© í˜•ì‹ ë³€í™˜
            m_fc2 = re.match(r'^(FC2)PPV(\d+)$', title, re.IGNORECASE)
            if m_fc2:
                title = f"{m_fc2.group(1).upper()}-PPV-{m_fc2.group(2)}"
            else:
                m_3prefix = re.match(r'^(\d{3})([A-Za-z]+)(\d+)$', title)
                if m_3prefix:
                    title = f"{m_3prefix.group(1)}{m_3prefix.group(2)}-{m_3prefix.group(3)}"
                elif len(title) <= 8:
                    m = re.match(r'^([A-Za-z]+)(\d+)$', title)
                    if m:
                        title = f"{m.group(1)}-{m.group(2)}"

            post_url = title_elem['href']
            if not post_url.startswith('http'):
                post_url = f"{BASE_URL}{post_url}"

            img_elem = card.find('img', class_='image')
            if not img_elem or 'src' not in img_elem.attrs:
                return None
            image_url = img_elem['src']

            size_elem = card.find('span', class_='is-size-6')
            if not size_elem:
                return None
            file_size = size_elem.text.strip()

            tag_elems = card.find_all('a', class_='tag')
            tags = [tag.text.strip() for tag in tag_elems]

            download_elem = card.find('a', class_='button is-primary is-fullwidth')
            if not download_elem or 'href' not in download_elem.attrs:
                return None
            download_url = download_elem['href']
            if not download_url.startswith('http'):
                download_url = f"{BASE_URL}{download_url}"

            # description ì¶”ì¶œ
            description = ""
            desc_elem = card.find('p', class_='level has-text-grey-dark')
            if desc_elem:
                description = desc_elem.text.strip()

            # actress ì¶”ì¶œ
            actress = []
            panel_elem = card.find('div', class_='panel')
            if panel_elem:
                actress = [a.text.strip() for a in panel_elem.find_all('a', class_='panel-block')]

            return {
                'url': post_url,
                'code': title,
                'title': title,
                'image_url': image_url,
                'file_size': file_size,
                'post_date': post_date,
                'tags': json.dumps(tags),
                'description': description,
                'translated_desc': "",  # ë²ˆì—­ì€ ë‚˜ì¤‘ì— ìˆ˜í–‰
                'actress': json.dumps(actress, ensure_ascii=False),
                'download_url': download_url,
                'scraped_at': datetime.now()
            }
        except Exception as e:
            print(f"ğŸ’¥ ê²Œì‹œë¬¼ ì²˜ë¦¬ ì˜¤ë¥˜: {str(e)}")
            return None

    def save_post(self, post_data: dict, skip_duplicate_check: bool = False):
        """ê²Œì‹œë¬¼ ë°ì´í„°ë¥¼ DBì— ì €ì¥"""
        try:
            # ì¤‘ë³µ ì²´í¬ (skip_duplicate_checkê°€ Falseì¼ ë•Œë§Œ)
            if not skip_duplicate_check and self.is_duplicate(post_data):
                return False

            # ë²ˆì—­ ìˆ˜í–‰
            if post_data['description']:
                try:
                    post_data['translated_desc'] = translate_to_korean(post_data['description'])
                except Exception as e:
                    print(f"ğŸ’¥ ë²ˆì—­ ì˜¤ë¥˜: {e}")
                    post_data['translated_desc'] = ""

            # ì €ì¥
            with self.lock:  # ìŠ¤ë ˆë“œ ì•ˆì „ì„±ì„ ìœ„í•œ ë½ ì‚¬ìš©
                if self.db.add_post(post_data):
                    print(f"ğŸ’« ì €ì¥ ì™„ë£Œ: {post_data['title']}")
                    return True
                else:
                    print(f"ğŸ’¥ ì €ì¥ ì‹¤íŒ¨: {post_data['title']}")
                    return False
        except Exception as e:
            print(f"ğŸ’¥ ì €ì¥ ì˜¤ë¥˜: {str(e)}")
            raise

    def get_next_page_url(self, soup: BeautifulSoup) -> Optional[str]:
        """ë‹¤ìŒ í˜ì´ì§€ URLì„ ì¶”ì¶œ"""
        try:
            pagination = soup.find('nav', class_='pagination')
            if not pagination:
                print("âš ï¸ í˜ì´ì§€ë„¤ì´ì…˜ ì—†ìŒ")
                return None
            next_link = pagination.find('a', class_='pagination-next')
            if not next_link or 'href' not in next_link.attrs:
                print("âš ï¸ ë‹¤ìŒ í˜ì´ì§€ ë§í¬ ì—†ìŒ")
                return None
            next_url = next_link['href']
            if not next_url.startswith('http'):
                if next_url.startswith('?'):
                    next_url = f"{BASE_URL}/new{next_url}"
                else:
                    next_url = f"{BASE_URL}{next_url}"
            print(f"â¡ï¸ ë‹¤ìŒ í˜ì´ì§€: {next_url}")
            return next_url
        except Exception as e:
            print(f"ğŸ’¥ URL ì¶”ì¶œ ì˜¤ë¥˜: {str(e)}")
        return None

    def scrape_new_posts(self):
        """ìƒˆë¡œìš´ ê²Œì‹œë¬¼ì„ ìŠ¤í¬ë˜í•‘"""
        start_time = time.time()
        print("ğŸš€ ìŠ¤í¬ë˜í•‘ ì‹œì‘")
        current_url = f"{BASE_URL}/new"
        yesterday = datetime.now().date() - timedelta(days=1)
        should_continue = True
        last_page_posts = []
        all_posts = []
        duplicate_count = 0
        saved_count = 0
        
        while current_url and should_continue:
            print(f"ğŸ“š í˜ì´ì§€ ì²˜ë¦¬: {current_url}")
            soup = self.get_page_with_selenium(current_url)
            
            cards = soup.find_all('div', class_='card mb-3')
            print(f"ğŸ”® ë°œê²¬ëœ ê²Œì‹œë¬¼: {len(cards)}ê°œ")
            
            # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê²Œì‹œë¬¼ ë°ì´í„° ì¶”ì¶œ
            with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
                future_to_card = {executor.submit(self.process_card, card): card for card in cards}
                page_posts = []
                
                for future in as_completed(future_to_card):
                    post_data = future.result()
                    if post_data:
                        if post_data['post_date'].date() < yesterday:
                            should_continue = False
                            print("â³ ì–´ì œ ì´ì „ ê²Œì‹œë¬¼ ë°œê²¬")
                            break
                        page_posts.append(post_data)
                        print(f"âœ¨ ê²Œì‹œë¬¼ ì²˜ë¦¬: {post_data['title']}")
            
            all_posts.extend(page_posts)
            if not should_continue:
                last_page_posts = page_posts
                break
                
            current_url = self.get_next_page_url(soup)
            if not current_url:
                print("ğŸ¯ ë§ˆì§€ë§‰ í˜ì´ì§€ ë„ë‹¬")
                last_page_posts = page_posts
                break

        # ë§ˆì§€ë§‰ í˜ì´ì§€ ê²Œì‹œë¬¼ ì €ì¥ (ì¤‘ë³µ ì²´í¬ ìˆ˜í–‰)
        for post_data in last_page_posts:
            if self.save_post(post_data, skip_duplicate_check=False):
                saved_count += 1
            else:
                duplicate_count += 1
        
        # ë‚˜ë¨¸ì§€ ê²Œì‹œë¬¼ ì €ì¥ (ì¤‘ë³µ ì²´í¬ ê±´ë„ˆë›°ê¸°)
        for post_data in all_posts:
            if post_data['url'] not in set(post['url'] for post in last_page_posts):
                if self.save_post(post_data, skip_duplicate_check=True):
                    saved_count += 1
        
        end_time = time.time()
        print(f"ğŸ”„ ì¤‘ë³µ ê²Œì‹œë¬¼: {duplicate_count}ê°œ")
        print(f"ğŸ’« ì €ì¥ëœ ê²Œì‹œë¬¼: {saved_count}ê°œ")
        print(f"ğŸŒŸ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ ({end_time - start_time:.1f}ì´ˆ)")

    def close(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        if hasattr(self, 'driver'):
            self.driver.quit() 